{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction from Bytes and Asm Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dataset containing .bytes and .asm files for each data point, we will be extracting features out of the two sets of files.\n",
    "\n",
    "Directory Containing Data:\n",
    "\n",
    "**base_dir : External_drive/data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction from .bytes files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source of 10868 .bytes files :- byte_dir : base_dir/byte_files**\n",
    "\n",
    "Features to be extracted from the .bytes files:\n",
    "\n",
    "1. .bytes file size\n",
    "2. entropy of the .bytes file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Extracting .bytes file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Script source to extract file sizes from the .bytes files\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from constants import *\t\t# for constants like base_dir, byte dir # Update this\n",
    "\n",
    "\n",
    "def ext_bytesize(source):\n",
    "    '''\n",
    "        Function to extract file size features from a source directory \n",
    "        containing byte files.\n",
    "        Build a csv file with id,size.\n",
    "    '''\n",
    "\n",
    "    if os.path.isdir(source):\n",
    "        sizes = dict()\t# dict to hold Id, size pairs\n",
    "        sizes['Id'] = 'Size'\t# For the first row in the csv file. to be used as the column names in the dataframe.\n",
    "        for name in tqdm(os.listdir(source)):\n",
    "            file_sizemb = os.stat(os.path.join(source, name)).st_size\n",
    "            file_sizemb /= ((1024.0)*(1024.0))\n",
    "            sizes[name.split('.')[0]] = file_sizemb\t\t# Each row in the dict->Series object->csv file->dataframe object\n",
    "        byte_sizes = pd.Series(sizes)\n",
    "        byte_sizes.to_csv(os.path.join(base_dir, 'bytef_size.csv')) \t# File to contain byte file size features\n",
    "    else:\n",
    "        print(f\"{source} directory doesn't exist.\")\n",
    "\n",
    "        \n",
    "############# Uncomment Here to run ##############\n",
    "def main():\n",
    "    #source = os.path.join(base_dir, byte_dir)\n",
    "    #ext_bytesize(source)\n",
    "    print('hello')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv generated above will be stored in base_dir.\n",
    "\n",
    "For further processing, the csv generated will be moved to present_working_directory/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01azqd4InC7m9JpocGv5</td>\n",
       "      <td>5.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01IsoiSMh5gxyDYTl4CB</td>\n",
       "      <td>6.556152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01jsnpXSAlgw6aPeDxrU</td>\n",
       "      <td>4.602051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01kcPWA9K2BOxQeS5Rju</td>\n",
       "      <td>0.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01SuzwMJEIXsK7A8dQbl</td>\n",
       "      <td>0.438965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Id      Size\n",
       "0  01azqd4InC7m9JpocGv5  5.012695\n",
       "1  01IsoiSMh5gxyDYTl4CB  6.556152\n",
       "2  01jsnpXSAlgw6aPeDxrU  4.602051\n",
       "3  01kcPWA9K2BOxQeS5Rju  0.679688\n",
       "4  01SuzwMJEIXsK7A8dQbl  0.438965"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploading the byte files sizes\n",
    "byte_size = pd.read_csv('./input/bytef_size.csv')\n",
    "byte_size.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Extracting file Entropy out of the .bytes files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy**\n",
    "\n",
    "Information entropy is the average rate at which information is produced by a stochastic source of data.\n",
    "When the data source produces a low-probability value (i.e., when a low-probability event occurs), the event carries more \"information\" (\"surprisal\") than when the source data produces a high-probability value. The amount of information conveyed by each event defined in this way becomes a random variable whose expected value is the information entropy. \n",
    "\n",
    "*Source: https://en.wikipedia.org/wiki/Entropy_(information_theory)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Entropy instead of count vectors?**\n",
    "\n",
    "Computing count vectors is one very obvious and straighforward apporoach for feature extraction from the .bytes files. \n",
    "From the first cut solution, we observed that the unigram based count vectors of 256 Dimensions can only give us a best\n",
    "log-loss of 0.03 and to imporve it further, an obviuos extension is to compute bi-grams or n-grams from the .bytes files.\n",
    "\n",
    "But, computing even just the bigrams for 10868 data points will take up about (10^4)*(256*256) --- this much memory.\n",
    "A good alternative to using the 256+ Dimensional Vectors is to use the concept of Entropy. \n",
    "Entropy of the file will be calculated using the information from the unigrams itself. Thus, giving us a way to represent \n",
    "all the file information from 256+ Dimensional vectors to a single numerical value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to calculate File Entropy?**\n",
    "\n",
    "File entropies can be calculated using the already computed unigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Script source to build unigram count vectors from the .bytes files\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from constants import * # todo\n",
    "\n",
    "\n",
    "def generate_keys():\n",
    "    '''\n",
    "        Function to generate the list_of_keys for building the feature_matrix\n",
    "        keys are like- '0', '1', '2', ... ,'5a',... 'cf', ...., 'ff' \n",
    "    '''\n",
    "    keys = [hex(i)[2:] for i in range(256)]\n",
    "    keys.insert(0, 'Id')\n",
    "    keys.append('??')\n",
    "    return keys\n",
    "\n",
    "\n",
    "def build_feature_matrix(source):\n",
    "    '''\n",
    "        Function reads a batch of byte files from the source directory.\n",
    "        Builds a count vector of 256(+1) Dimensions for each data point.\n",
    "        Thus effectively building the whole feature matrix for the whole dataset. \n",
    "        \n",
    "        This is a Custom Built BoW Vectorizer.\n",
    "\n",
    "        Returns: a list of dicts where each dict represents a data point.\n",
    "    ''' \n",
    "    count_vects = []\n",
    "    for file in tqdm(os.listdir(source)):\n",
    "        \n",
    "        # Initialising the dict/count_vector for this file\n",
    "        list_of_keys = generate_keys()\n",
    "        d = {key: 0 for key in list_of_keys}\n",
    "        d['Id'] = file.split('.')[0]\n",
    "\n",
    "        # Building the dict/count_vector for this file\n",
    "        with open(os.path.join(source, file), 'r') as f:\n",
    "        # Iterating over each line in the file\n",
    "            for line in f.read().split('\\n'):\n",
    "                for byte in line.split()[1:]:\n",
    "                    # a byte here will be initially in hex code\n",
    "                    # to solve the issue of '08' appearing in file, \n",
    "                    # we will convert this hex code into int and then\n",
    "                    # back to hex. This gives=> \n",
    "                    # '08 from file'->8 on dec->'8' on hex(this is what is found in keys)\n",
    "                    if not byte == '??':\n",
    "                        byte = int(byte, base=16)\n",
    "                        d[hex(byte)[2:]] += 1\n",
    "                    else:\n",
    "                        d[byte] += 1\n",
    "\n",
    "        # Appending this count_vect/dict in the list of dicts\t\n",
    "        count_vects.append(d)\n",
    "\n",
    "    return count_vects\n",
    "\n",
    "\n",
    "def build_csv(feature_matrix):\n",
    "    '''\n",
    "        Function to save the feature matrix in csv file\n",
    "        Input: list of dict where each dict represents a data point\n",
    "    '''\n",
    "    df = pd.DataFrame(feature_matrix)\n",
    "    df = df.set_index('Id')\n",
    "    destination = os.path.join(base_dir, 'byte_features.csv')  # 257-D vectors to be stored in base_dir/byte_features.csv\n",
    "    if os.path.exists(destination):\n",
    "        os.remove(destination)\n",
    "    df.to_csv(destination) \n",
    "\n",
    "\n",
    "    \n",
    "################# To Run, Uncomment here ##############################\n",
    "def main():\n",
    "    #source = byte_dir  # byte_dir stored in constants.py ; contains base_dir/byte_files\n",
    "    #source = sample_byte_dir # test code\n",
    "    #feature_matrix = build_feature_matrix(source)\n",
    "    #build_csv(feature_matrix)\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>fa</th>\n",
       "      <th>fb</th>\n",
       "      <th>fc</th>\n",
       "      <th>fd</th>\n",
       "      <th>fe</th>\n",
       "      <th>ff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01azqd4InC7m9JpocGv5</td>\n",
       "      <td>601905</td>\n",
       "      <td>3905</td>\n",
       "      <td>3269</td>\n",
       "      <td>2740</td>\n",
       "      <td>3459</td>\n",
       "      <td>2767</td>\n",
       "      <td>3354</td>\n",
       "      <td>4049</td>\n",
       "      <td>3184</td>\n",
       "      <td>...</td>\n",
       "      <td>3271</td>\n",
       "      <td>2804</td>\n",
       "      <td>3687</td>\n",
       "      <td>3101</td>\n",
       "      <td>3211</td>\n",
       "      <td>3097</td>\n",
       "      <td>2758</td>\n",
       "      <td>3099</td>\n",
       "      <td>2759</td>\n",
       "      <td>5753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01IsoiSMh5gxyDYTl4CB</td>\n",
       "      <td>39755</td>\n",
       "      <td>8337</td>\n",
       "      <td>8869</td>\n",
       "      <td>228</td>\n",
       "      <td>352581</td>\n",
       "      <td>187</td>\n",
       "      <td>12034</td>\n",
       "      <td>353</td>\n",
       "      <td>349</td>\n",
       "      <td>...</td>\n",
       "      <td>290</td>\n",
       "      <td>451</td>\n",
       "      <td>6536</td>\n",
       "      <td>439</td>\n",
       "      <td>281</td>\n",
       "      <td>302</td>\n",
       "      <td>7639</td>\n",
       "      <td>518</td>\n",
       "      <td>17001</td>\n",
       "      <td>54902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01jsnpXSAlgw6aPeDxrU</td>\n",
       "      <td>93506</td>\n",
       "      <td>9542</td>\n",
       "      <td>2782</td>\n",
       "      <td>2611</td>\n",
       "      <td>2301</td>\n",
       "      <td>2706</td>\n",
       "      <td>2968</td>\n",
       "      <td>2808</td>\n",
       "      <td>2408</td>\n",
       "      <td>...</td>\n",
       "      <td>2598</td>\n",
       "      <td>2325</td>\n",
       "      <td>2358</td>\n",
       "      <td>2242</td>\n",
       "      <td>2885</td>\n",
       "      <td>2863</td>\n",
       "      <td>2471</td>\n",
       "      <td>2786</td>\n",
       "      <td>2680</td>\n",
       "      <td>49144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01kcPWA9K2BOxQeS5Rju</td>\n",
       "      <td>21091</td>\n",
       "      <td>1213</td>\n",
       "      <td>1067</td>\n",
       "      <td>407</td>\n",
       "      <td>389</td>\n",
       "      <td>423</td>\n",
       "      <td>644</td>\n",
       "      <td>520</td>\n",
       "      <td>413</td>\n",
       "      <td>...</td>\n",
       "      <td>446</td>\n",
       "      <td>478</td>\n",
       "      <td>873</td>\n",
       "      <td>485</td>\n",
       "      <td>462</td>\n",
       "      <td>516</td>\n",
       "      <td>1133</td>\n",
       "      <td>471</td>\n",
       "      <td>761</td>\n",
       "      <td>7998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01SuzwMJEIXsK7A8dQbl</td>\n",
       "      <td>19764</td>\n",
       "      <td>710</td>\n",
       "      <td>447</td>\n",
       "      <td>242</td>\n",
       "      <td>240</td>\n",
       "      <td>236</td>\n",
       "      <td>437</td>\n",
       "      <td>693</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>847</td>\n",
       "      <td>947</td>\n",
       "      <td>350</td>\n",
       "      <td>209</td>\n",
       "      <td>239</td>\n",
       "      <td>653</td>\n",
       "      <td>221</td>\n",
       "      <td>242</td>\n",
       "      <td>2199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Id       0     1    10    11      12    13     14    15  \\\n",
       "0  01azqd4InC7m9JpocGv5  601905  3905  3269  2740    3459  2767   3354  4049   \n",
       "1  01IsoiSMh5gxyDYTl4CB   39755  8337  8869   228  352581   187  12034   353   \n",
       "2  01jsnpXSAlgw6aPeDxrU   93506  9542  2782  2611    2301  2706   2968  2808   \n",
       "3  01kcPWA9K2BOxQeS5Rju   21091  1213  1067   407     389   423    644   520   \n",
       "4  01SuzwMJEIXsK7A8dQbl   19764   710   447   242     240   236    437   693   \n",
       "\n",
       "     16  ...    f6    f7    f8    f9    fa    fb    fc    fd     fe     ff  \n",
       "0  3184  ...  3271  2804  3687  3101  3211  3097  2758  3099   2759   5753  \n",
       "1   349  ...   290   451  6536   439   281   302  7639   518  17001  54902  \n",
       "2  2408  ...  2598  2325  2358  2242  2885  2863  2471  2786   2680  49144  \n",
       "3   413  ...   446   478   873   485   462   516  1133   471    761   7998  \n",
       "4   240  ...   483   847   947   350   209   239   653   221    242   2199  \n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the unigrams\n",
    "byte_unigrams = pd.read_csv('./input/byte_features.csv')\n",
    "byte_unigrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Once the unigrams have been computed, we can calculate the file entropies.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Script source to extract file entropy from the .bytes files\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from constants import * # do something about this. constants is a custom python module containing all constant directories\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def get_file_entropy(obj):\n",
    "    '''\n",
    "        Function to calculate file entropy given a pandas series object representing the \n",
    "        257 dimensional count vector for the file. \n",
    "        \n",
    "        Input: A series object as a count vector for a .byte file\n",
    "        Return: entropy calculated for the file\n",
    "    '''\n",
    "    obj = obj[1:] # dropping off 'Id' feature for entropy calculation\n",
    "    #total_non_zero = obj.to_numpy().nonzero()[0].shape[0]\n",
    "    total_sum = obj.sum() \t# Correction made here\n",
    "    #print(total_non_zero, ' in this file')\n",
    "    entropy = 0.0\n",
    "    for byte_code in obj.index:\n",
    "        if byte_code != 'Id' and obj[byte_code]:\n",
    "            probab = obj[byte_code] / total_sum    # Correction made here\n",
    "            entropy -= probab * math.log(probab)\n",
    "    #print('entropy = ', entropy)\n",
    "    return entropy\n",
    "\n",
    "def build_entropy_csv(source):\n",
    "    '''\n",
    "        Function to read the csv containing the 257 dimensional count vectors for each of the 10868 data point and\n",
    "        calculating the byte file entropy for each data point using the 257 dimensional count vectors.\n",
    "        \n",
    "        Input: Source path to the csv file containing the count vecs of the 10868 .bytes files\n",
    "    '''\n",
    "    df = pd.read_csv(source)\n",
    "    df['entropy'] = 0.0\n",
    "    entropies = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        entropies.append(get_file_entropy(row))\n",
    "\n",
    "    df['entropy'] = entropies\n",
    "    df = df.set_index('Id')\n",
    "    df.to_csv(os.path.join(base_dir, 'byte_features_entropy.csv'))  # Saving the entropies of the files in the csv in base_dir\n",
    "\n",
    "\n",
    "\n",
    "######## Uncomment here to Run #############\n",
    "def main():\n",
    "    #source = os.path.join(base_dir, 'byte_features.csv') # byte_features.csv contains the computed count vecs\n",
    "    #build_entropy_csv(source)\n",
    "    print('hello')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>fa</th>\n",
       "      <th>fb</th>\n",
       "      <th>fc</th>\n",
       "      <th>fd</th>\n",
       "      <th>fe</th>\n",
       "      <th>ff</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01azqd4InC7m9JpocGv5</td>\n",
       "      <td>601905</td>\n",
       "      <td>3905</td>\n",
       "      <td>3269</td>\n",
       "      <td>2740</td>\n",
       "      <td>3459</td>\n",
       "      <td>2767</td>\n",
       "      <td>3354</td>\n",
       "      <td>4049</td>\n",
       "      <td>3184</td>\n",
       "      <td>...</td>\n",
       "      <td>2804</td>\n",
       "      <td>3687</td>\n",
       "      <td>3101</td>\n",
       "      <td>3211</td>\n",
       "      <td>3097</td>\n",
       "      <td>2758</td>\n",
       "      <td>3099</td>\n",
       "      <td>2759</td>\n",
       "      <td>5753</td>\n",
       "      <td>3.912142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01IsoiSMh5gxyDYTl4CB</td>\n",
       "      <td>39755</td>\n",
       "      <td>8337</td>\n",
       "      <td>8869</td>\n",
       "      <td>228</td>\n",
       "      <td>352581</td>\n",
       "      <td>187</td>\n",
       "      <td>12034</td>\n",
       "      <td>353</td>\n",
       "      <td>349</td>\n",
       "      <td>...</td>\n",
       "      <td>451</td>\n",
       "      <td>6536</td>\n",
       "      <td>439</td>\n",
       "      <td>281</td>\n",
       "      <td>302</td>\n",
       "      <td>7639</td>\n",
       "      <td>518</td>\n",
       "      <td>17001</td>\n",
       "      <td>54902</td>\n",
       "      <td>3.421943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01jsnpXSAlgw6aPeDxrU</td>\n",
       "      <td>93506</td>\n",
       "      <td>9542</td>\n",
       "      <td>2782</td>\n",
       "      <td>2611</td>\n",
       "      <td>2301</td>\n",
       "      <td>2706</td>\n",
       "      <td>2968</td>\n",
       "      <td>2808</td>\n",
       "      <td>2408</td>\n",
       "      <td>...</td>\n",
       "      <td>2325</td>\n",
       "      <td>2358</td>\n",
       "      <td>2242</td>\n",
       "      <td>2885</td>\n",
       "      <td>2863</td>\n",
       "      <td>2471</td>\n",
       "      <td>2786</td>\n",
       "      <td>2680</td>\n",
       "      <td>49144</td>\n",
       "      <td>4.471821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01kcPWA9K2BOxQeS5Rju</td>\n",
       "      <td>21091</td>\n",
       "      <td>1213</td>\n",
       "      <td>1067</td>\n",
       "      <td>407</td>\n",
       "      <td>389</td>\n",
       "      <td>423</td>\n",
       "      <td>644</td>\n",
       "      <td>520</td>\n",
       "      <td>413</td>\n",
       "      <td>...</td>\n",
       "      <td>478</td>\n",
       "      <td>873</td>\n",
       "      <td>485</td>\n",
       "      <td>462</td>\n",
       "      <td>516</td>\n",
       "      <td>1133</td>\n",
       "      <td>471</td>\n",
       "      <td>761</td>\n",
       "      <td>7998</td>\n",
       "      <td>4.952995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01SuzwMJEIXsK7A8dQbl</td>\n",
       "      <td>19764</td>\n",
       "      <td>710</td>\n",
       "      <td>447</td>\n",
       "      <td>242</td>\n",
       "      <td>240</td>\n",
       "      <td>236</td>\n",
       "      <td>437</td>\n",
       "      <td>693</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>847</td>\n",
       "      <td>947</td>\n",
       "      <td>350</td>\n",
       "      <td>209</td>\n",
       "      <td>239</td>\n",
       "      <td>653</td>\n",
       "      <td>221</td>\n",
       "      <td>242</td>\n",
       "      <td>2199</td>\n",
       "      <td>4.816877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Id       0     1    10    11      12    13     14    15  \\\n",
       "0  01azqd4InC7m9JpocGv5  601905  3905  3269  2740    3459  2767   3354  4049   \n",
       "1  01IsoiSMh5gxyDYTl4CB   39755  8337  8869   228  352581   187  12034   353   \n",
       "2  01jsnpXSAlgw6aPeDxrU   93506  9542  2782  2611    2301  2706   2968  2808   \n",
       "3  01kcPWA9K2BOxQeS5Rju   21091  1213  1067   407     389   423    644   520   \n",
       "4  01SuzwMJEIXsK7A8dQbl   19764   710   447   242     240   236    437   693   \n",
       "\n",
       "     16  ...    f7    f8    f9    fa    fb    fc    fd     fe     ff   entropy  \n",
       "0  3184  ...  2804  3687  3101  3211  3097  2758  3099   2759   5753  3.912142  \n",
       "1   349  ...   451  6536   439   281   302  7639   518  17001  54902  3.421943  \n",
       "2  2408  ...  2325  2358  2242  2885  2863  2471  2786   2680  49144  4.471821  \n",
       "3   413  ...   478   873   485   462   516  1133   471    761   7998  4.952995  \n",
       "4   240  ...   847   947   350   209   239   653   221    242   2199  4.816877  \n",
       "\n",
       "[5 rows x 259 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the entropies\n",
    "byte_entropy = pd.read_csv('./input/byte_features_entropy.csv')\n",
    "byte_entropy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Combining byte features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the byte file sizes with the byte file entropies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>fa</th>\n",
       "      <th>fb</th>\n",
       "      <th>fc</th>\n",
       "      <th>fd</th>\n",
       "      <th>fe</th>\n",
       "      <th>ff</th>\n",
       "      <th>entropy</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01azqd4InC7m9JpocGv5</td>\n",
       "      <td>601905</td>\n",
       "      <td>3905</td>\n",
       "      <td>3269</td>\n",
       "      <td>2740</td>\n",
       "      <td>3459</td>\n",
       "      <td>2767</td>\n",
       "      <td>3354</td>\n",
       "      <td>4049</td>\n",
       "      <td>3184</td>\n",
       "      <td>...</td>\n",
       "      <td>3687</td>\n",
       "      <td>3101</td>\n",
       "      <td>3211</td>\n",
       "      <td>3097</td>\n",
       "      <td>2758</td>\n",
       "      <td>3099</td>\n",
       "      <td>2759</td>\n",
       "      <td>5753</td>\n",
       "      <td>3.912142</td>\n",
       "      <td>5.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01IsoiSMh5gxyDYTl4CB</td>\n",
       "      <td>39755</td>\n",
       "      <td>8337</td>\n",
       "      <td>8869</td>\n",
       "      <td>228</td>\n",
       "      <td>352581</td>\n",
       "      <td>187</td>\n",
       "      <td>12034</td>\n",
       "      <td>353</td>\n",
       "      <td>349</td>\n",
       "      <td>...</td>\n",
       "      <td>6536</td>\n",
       "      <td>439</td>\n",
       "      <td>281</td>\n",
       "      <td>302</td>\n",
       "      <td>7639</td>\n",
       "      <td>518</td>\n",
       "      <td>17001</td>\n",
       "      <td>54902</td>\n",
       "      <td>3.421943</td>\n",
       "      <td>6.556152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01jsnpXSAlgw6aPeDxrU</td>\n",
       "      <td>93506</td>\n",
       "      <td>9542</td>\n",
       "      <td>2782</td>\n",
       "      <td>2611</td>\n",
       "      <td>2301</td>\n",
       "      <td>2706</td>\n",
       "      <td>2968</td>\n",
       "      <td>2808</td>\n",
       "      <td>2408</td>\n",
       "      <td>...</td>\n",
       "      <td>2358</td>\n",
       "      <td>2242</td>\n",
       "      <td>2885</td>\n",
       "      <td>2863</td>\n",
       "      <td>2471</td>\n",
       "      <td>2786</td>\n",
       "      <td>2680</td>\n",
       "      <td>49144</td>\n",
       "      <td>4.471821</td>\n",
       "      <td>4.602051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01kcPWA9K2BOxQeS5Rju</td>\n",
       "      <td>21091</td>\n",
       "      <td>1213</td>\n",
       "      <td>1067</td>\n",
       "      <td>407</td>\n",
       "      <td>389</td>\n",
       "      <td>423</td>\n",
       "      <td>644</td>\n",
       "      <td>520</td>\n",
       "      <td>413</td>\n",
       "      <td>...</td>\n",
       "      <td>873</td>\n",
       "      <td>485</td>\n",
       "      <td>462</td>\n",
       "      <td>516</td>\n",
       "      <td>1133</td>\n",
       "      <td>471</td>\n",
       "      <td>761</td>\n",
       "      <td>7998</td>\n",
       "      <td>4.952995</td>\n",
       "      <td>0.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01SuzwMJEIXsK7A8dQbl</td>\n",
       "      <td>19764</td>\n",
       "      <td>710</td>\n",
       "      <td>447</td>\n",
       "      <td>242</td>\n",
       "      <td>240</td>\n",
       "      <td>236</td>\n",
       "      <td>437</td>\n",
       "      <td>693</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>947</td>\n",
       "      <td>350</td>\n",
       "      <td>209</td>\n",
       "      <td>239</td>\n",
       "      <td>653</td>\n",
       "      <td>221</td>\n",
       "      <td>242</td>\n",
       "      <td>2199</td>\n",
       "      <td>4.816877</td>\n",
       "      <td>0.438965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Id       0     1    10    11      12    13     14    15  \\\n",
       "0  01azqd4InC7m9JpocGv5  601905  3905  3269  2740    3459  2767   3354  4049   \n",
       "1  01IsoiSMh5gxyDYTl4CB   39755  8337  8869   228  352581   187  12034   353   \n",
       "2  01jsnpXSAlgw6aPeDxrU   93506  9542  2782  2611    2301  2706   2968  2808   \n",
       "3  01kcPWA9K2BOxQeS5Rju   21091  1213  1067   407     389   423    644   520   \n",
       "4  01SuzwMJEIXsK7A8dQbl   19764   710   447   242     240   236    437   693   \n",
       "\n",
       "     16  ...    f8    f9    fa    fb    fc    fd     fe     ff   entropy  \\\n",
       "0  3184  ...  3687  3101  3211  3097  2758  3099   2759   5753  3.912142   \n",
       "1   349  ...  6536   439   281   302  7639   518  17001  54902  3.421943   \n",
       "2  2408  ...  2358  2242  2885  2863  2471  2786   2680  49144  4.471821   \n",
       "3   413  ...   873   485   462   516  1133   471    761   7998  4.952995   \n",
       "4   240  ...   947   350   209   239   653   221    242   2199  4.816877   \n",
       "\n",
       "       Size  \n",
       "0  5.012695  \n",
       "1  6.556152  \n",
       "2  4.602051  \n",
       "3  0.679688  \n",
       "4  0.438965  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging the dataframes\n",
    "byte_final = byte_entropy.merge(byte_size, on='Id')\n",
    "byte_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving into csv\n",
    "byte_final_tosave = byte_final.set_index('Id')\n",
    "byte_final_tosave.to_csv('./input/byte_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>fa</th>\n",
       "      <th>fb</th>\n",
       "      <th>fc</th>\n",
       "      <th>fd</th>\n",
       "      <th>fe</th>\n",
       "      <th>ff</th>\n",
       "      <th>entropy</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01azqd4InC7m9JpocGv5</td>\n",
       "      <td>601905</td>\n",
       "      <td>3905</td>\n",
       "      <td>3269</td>\n",
       "      <td>2740</td>\n",
       "      <td>3459</td>\n",
       "      <td>2767</td>\n",
       "      <td>3354</td>\n",
       "      <td>4049</td>\n",
       "      <td>3184</td>\n",
       "      <td>...</td>\n",
       "      <td>3687</td>\n",
       "      <td>3101</td>\n",
       "      <td>3211</td>\n",
       "      <td>3097</td>\n",
       "      <td>2758</td>\n",
       "      <td>3099</td>\n",
       "      <td>2759</td>\n",
       "      <td>5753</td>\n",
       "      <td>3.912142</td>\n",
       "      <td>5.012695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01IsoiSMh5gxyDYTl4CB</td>\n",
       "      <td>39755</td>\n",
       "      <td>8337</td>\n",
       "      <td>8869</td>\n",
       "      <td>228</td>\n",
       "      <td>352581</td>\n",
       "      <td>187</td>\n",
       "      <td>12034</td>\n",
       "      <td>353</td>\n",
       "      <td>349</td>\n",
       "      <td>...</td>\n",
       "      <td>6536</td>\n",
       "      <td>439</td>\n",
       "      <td>281</td>\n",
       "      <td>302</td>\n",
       "      <td>7639</td>\n",
       "      <td>518</td>\n",
       "      <td>17001</td>\n",
       "      <td>54902</td>\n",
       "      <td>3.421943</td>\n",
       "      <td>6.556152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01jsnpXSAlgw6aPeDxrU</td>\n",
       "      <td>93506</td>\n",
       "      <td>9542</td>\n",
       "      <td>2782</td>\n",
       "      <td>2611</td>\n",
       "      <td>2301</td>\n",
       "      <td>2706</td>\n",
       "      <td>2968</td>\n",
       "      <td>2808</td>\n",
       "      <td>2408</td>\n",
       "      <td>...</td>\n",
       "      <td>2358</td>\n",
       "      <td>2242</td>\n",
       "      <td>2885</td>\n",
       "      <td>2863</td>\n",
       "      <td>2471</td>\n",
       "      <td>2786</td>\n",
       "      <td>2680</td>\n",
       "      <td>49144</td>\n",
       "      <td>4.471821</td>\n",
       "      <td>4.602051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01kcPWA9K2BOxQeS5Rju</td>\n",
       "      <td>21091</td>\n",
       "      <td>1213</td>\n",
       "      <td>1067</td>\n",
       "      <td>407</td>\n",
       "      <td>389</td>\n",
       "      <td>423</td>\n",
       "      <td>644</td>\n",
       "      <td>520</td>\n",
       "      <td>413</td>\n",
       "      <td>...</td>\n",
       "      <td>873</td>\n",
       "      <td>485</td>\n",
       "      <td>462</td>\n",
       "      <td>516</td>\n",
       "      <td>1133</td>\n",
       "      <td>471</td>\n",
       "      <td>761</td>\n",
       "      <td>7998</td>\n",
       "      <td>4.952995</td>\n",
       "      <td>0.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01SuzwMJEIXsK7A8dQbl</td>\n",
       "      <td>19764</td>\n",
       "      <td>710</td>\n",
       "      <td>447</td>\n",
       "      <td>242</td>\n",
       "      <td>240</td>\n",
       "      <td>236</td>\n",
       "      <td>437</td>\n",
       "      <td>693</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>947</td>\n",
       "      <td>350</td>\n",
       "      <td>209</td>\n",
       "      <td>239</td>\n",
       "      <td>653</td>\n",
       "      <td>221</td>\n",
       "      <td>242</td>\n",
       "      <td>2199</td>\n",
       "      <td>4.816877</td>\n",
       "      <td>0.438965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Id       0     1    10    11      12    13     14    15  \\\n",
       "0  01azqd4InC7m9JpocGv5  601905  3905  3269  2740    3459  2767   3354  4049   \n",
       "1  01IsoiSMh5gxyDYTl4CB   39755  8337  8869   228  352581   187  12034   353   \n",
       "2  01jsnpXSAlgw6aPeDxrU   93506  9542  2782  2611    2301  2706   2968  2808   \n",
       "3  01kcPWA9K2BOxQeS5Rju   21091  1213  1067   407     389   423    644   520   \n",
       "4  01SuzwMJEIXsK7A8dQbl   19764   710   447   242     240   236    437   693   \n",
       "\n",
       "     16  ...    f8    f9    fa    fb    fc    fd     fe     ff   entropy  \\\n",
       "0  3184  ...  3687  3101  3211  3097  2758  3099   2759   5753  3.912142   \n",
       "1   349  ...  6536   439   281   302  7639   518  17001  54902  3.421943   \n",
       "2  2408  ...  2358  2242  2885  2863  2471  2786   2680  49144  4.471821   \n",
       "3   413  ...   873   485   462   516  1133   471    761   7998  4.952995   \n",
       "4   240  ...   947   350   209   239   653   221    242   2199  4.816877   \n",
       "\n",
       "       Size  \n",
       "0  5.012695  \n",
       "1  6.556152  \n",
       "2  4.602051  \n",
       "3  0.679688  \n",
       "4  0.438965  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the saved csv\n",
    "byte_final = pd.read_csv('./input/byte_final.csv')\n",
    "byte_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction from .asm files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain about the enormity of the asm files and that it requires parallel programming. \n",
    "\n",
    "For parallel programming, the asm files need to regrouped a couple of times into 3 or 4 different folders.\n",
    "\n",
    "**Base Source for asm files: asm_dir = base_dir/asm_files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Script source for grouping and regrouping the asm files\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from constants import *\n",
    "\n",
    "def divide_from(source):\n",
    "    all_files = os.listdir(source)\n",
    "    asm1 = os.path.join(base_dir, 'asm1')\n",
    "    asm2 = os.path.join(base_dir, 'asm2')\n",
    "    asm3 = os.path.join(base_dir, 'asm3')\n",
    "    asm4 = os.path.join(base_dir, 'asm4')\n",
    "\n",
    "    if not os.path.exists(asm1):\n",
    "        os.mkdir(asm1)\n",
    "\n",
    "    if not os.path.exists(asm2):\n",
    "        os.mkdir(asm2)\n",
    "\n",
    "    if not os.path.exists(asm3):\n",
    "        os.mkdir(asm3)\n",
    "\n",
    "    if not os.path.exists(asm4):\n",
    "        os.mkdir(asm4)\n",
    "    \n",
    "    current_dir = os.getcwd()\n",
    "    os.chdir(source)\t\n",
    "    for i in tqdm(range(10868)):\n",
    "        if i % 4 == 1:\n",
    "            shutil.move(os.path.join(source, all_files[i]), os.path.join(asm1, all_files[i]))\n",
    "        elif i % 4 == 2:\n",
    "            shutil.move(all_files[i], asm2)\n",
    "        elif i % 4 == 3:\n",
    "            shutil.move(all_files[i], asm3)\n",
    "        else:\n",
    "            shutil.move(all_files[i], asm4)\n",
    "    os.chdir(current_dir)\n",
    "    \n",
    "\n",
    "def copy_first20(source):\n",
    "    all_files = os.listdir(source)\n",
    "    current_dir = os.getcwd()\n",
    "    os.chdir(source)\t\n",
    "    for i in tqdm(range(20)):\n",
    "        shutil.copy(os.path.join(source, all_files[i]), os.path.join(sample_asm_dir, all_files[i]))\n",
    "    os.chdir(current_dir)\n",
    "\n",
    "def move_back(source_dirs, destination):\n",
    "    for source in tqdm(source_dirs):\n",
    "        for file in tqdm(os.listdir(source)):\n",
    "            shutil.move(os.path.join(source, file), os.path.join(destination, file))\n",
    "\n",
    "\n",
    "            \n",
    "############ Uncomment below to run #################\n",
    "def main():\n",
    "    #divide_from(asm_dir)\n",
    "    #copy_first20(asm_dir)\n",
    "    #asm_dirs = [os.path.join(base_dir, 'asm2'), os.path.join(base_dir, 'asm3')]\n",
    "    #move_back(asm_dirs, asm_dir)\n",
    "    print()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extracting and combining asm file size features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extracting the asm file size features, the asm files have been divided into 3 different directories:\n",
    "\n",
    "1. base_dir/asm1\n",
    "2. base_dir/asm2\n",
    "3. base_dir/asm3\n",
    "\n",
    "The script below is made to run on all of the above 3 directories to give 3 different csv files which will be combined to give\n",
    "the total asm files for 10868 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Script Source for extracting asm file size featues\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from constants import *\t\t# for constants like base_dir, byte dir\n",
    "\n",
    "\n",
    "def ext_asmsize(source, location_num):\n",
    "    '''\n",
    "        Function to extract file size features from a source directory \n",
    "        containing byte files.\n",
    "        Build a csv file with id,size.\n",
    "    '''\n",
    "\n",
    "    if os.path.isdir(source):\n",
    "        sizes = dict()\t# dict to hold Id, size pairs\n",
    "        sizes['Id'] = 'Size'\t# For the first row in the csv file. to be used as the column names in the dataframe.\n",
    "        for name in tqdm(os.listdir(source)):\n",
    "            file_sizemb = os.stat(os.path.join(source, name)).st_size\n",
    "            file_sizemb /= ((1024.0)*(1024.0))\n",
    "            sizes[name.split('.')[0]] = file_sizemb\t\t# Each row in the dict->Series object->csv file->dataframe object\n",
    "        asm_sizes = pd.Series(sizes)\n",
    "        asm_sizes.to_csv(os.path.join(base_dir, f'asm{location_num}_size.csv')) \t# File to contain byte file size features\n",
    "    else:\n",
    "        print(f\"{source} directory doesn't exist.\")\n",
    "\n",
    "        \n",
    "################# Uncomment below to run #############\n",
    "def main():\n",
    "    #asm1 = os.path.join(base_dir, 'asm1')\n",
    "    #asm2 = os.path.join(base_dir, 'asm2')\n",
    "    #asm3 = os.path.join(base_dir, 'asm3')\n",
    "    #asm_dirs = [asm1, asm2, asm3]\n",
    "    #for i in range(len(asm_dirs)):\n",
    "        #ext_asmsize(asm_dirs[i], i+1)\n",
    "    print()\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the asm file size csvs\n",
    "asm_size1 = pd.read_csv('./input/asm1_size.csv')\n",
    "asm_size2 = pd.read_csv('./input/asm2_size.csv')\n",
    "asm_size3 = pd.read_csv('./input/asm3_size.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01IsoiSMh5gxyDYTl4CB</td>\n",
       "      <td>13.999378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01SuzwMJEIXsK7A8dQbl</td>\n",
       "      <td>0.996723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02K5GMYITj7bBoAisEmD</td>\n",
       "      <td>5.488952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02zcUmKV16Lya5xqnPGB</td>\n",
       "      <td>36.020807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04cvLCVPqBMs6yn5xGlE</td>\n",
       "      <td>32.699180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Id       Size\n",
       "0  01IsoiSMh5gxyDYTl4CB  13.999378\n",
       "1  01SuzwMJEIXsK7A8dQbl   0.996723\n",
       "2  02K5GMYITj7bBoAisEmD   5.488952\n",
       "3  02zcUmKV16Lya5xqnPGB  36.020807\n",
       "4  04cvLCVPqBMs6yn5xGlE  32.699180"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining the asm file size features\n",
    "asm_size_features = pd.concat(objs=[asm_size1, asm_size2, asm_size3])\n",
    "asm_size_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extracting and Combining asm count vector features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the inspiration from the following source:\n",
    "\n",
    "We will be extracting the counts for about 150 tokens from the full set of asm files.\n",
    "Reading and Extracting such features out of about 150GB of asm files is a heavy process and thus shall be parallelized by first \n",
    "dividing the full asm files into 4 directories as:\n",
    "\n",
    "1. base_dir/asm1\n",
    "2. base_dir/asm2\n",
    "3. base_dir/asm3\n",
    "4. base_dir/asm4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Script source to extract count vector features out of the asm files directory\n",
    "# Warning: Heavy Code--> Took 27+ Hours to run on a moderate machine\n",
    "\n",
    "# v1.1 -> for large features extraction\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import time\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from constants import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process1():\n",
    "\n",
    "    source = os.path.join(base_dir, 'asm1')\n",
    "\n",
    "    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:'\\\n",
    "                ,'.tls:','.reloc:','.BSS:','.CODE']\n",
    "\n",
    "    opcodes = [\n",
    "        'add','al','bt','call','cdq','cld','cli','cmc','cmp', \\\n",
    "        'const','cwd','daa','db','dd','dec','dw','endp','ends',\\\n",
    "        'faddp','fchs','fdiv','fdivp','fdivr','fild','fistp','fld',\\\n",
    "        'fstcw','fstcwimul','fstp','fword','fxch','imul','in','inc','wait','xchg','xor'\\\n",
    "        'ins','int','jb','je','jg','jge','jl','jmp','jnb','jno','jnz','jo','jz',\\\n",
    "        'lea','loope','mov','movzx','mul','near','neg','not','or','out','outs',\\\n",
    "        'pop','popf','proc','push','pushf','rcl','rcr','rdtsc','rep','ret','retn',\\\n",
    "        'rol','ror','sal','sar','sbb','scas','setb','setle','setnle','setnz',\\\n",
    "        'setz','shl','shld','shr','sidt','stc','std','sti','stos','sub','test',\n",
    "    ]\n",
    "\n",
    "\n",
    "    keywords = ['.dll','std::',':dword', 'Virtual', 'Offset', 'loc', 'ptr', 'data', 'FUNCTION', 'void', 'byte', 'trap', \\\n",
    "                'arg', 'dll', 'stdcall', 'asc', 'align', 'rva', 'proc', 'assume', 'Import', 'extrn', \n",
    "    ]\n",
    "\n",
    "\n",
    "    # virtual in small\n",
    "    # offset in small\n",
    "\n",
    "\n",
    "    registers = [\n",
    "        'edx', 'esi', 'es', 'fs', 'ds', 'ss', 'gs', 'cs', 'ah', 'al', 'ax', 'bh', 'bl', 'bx', 'ch', 'cl',\\\n",
    "        'cx', 'dh', 'dl', 'dx', 'eax', 'ebp', 'ebx', 'ecx', 'edi', 'esp', 'eip'\n",
    "    ]\n",
    "\n",
    "    keys = ['Id'] + prefixes + opcodes + keywords + registers\n",
    "\n",
    "\n",
    "    count_vects = []\n",
    "    for file in tqdm(os.listdir(source)):\n",
    "        \n",
    "        # Initialising the dict/count_vector for this file\n",
    "        d = {key: 0 for key in keys}\n",
    "        d['Id'] = file.split('.')[0]\n",
    "\n",
    "        # Building the dict/count_vector for this file\n",
    "        with codecs.open(os.path.join(source, file), mode='r', encoding='cp1252', errors='ignore') as f:\n",
    "            # Iterating over each line in the file\n",
    "            for line in f.read().split('\\n'):\n",
    "                words = line.split()\n",
    "                if not len(words):\n",
    "                    continue\n",
    "                prefix_container = words[0]\n",
    "                rem_line = words[1:]\n",
    "\n",
    "                # Checking what prefix is there in the container\n",
    "                for prefix in prefixes:\n",
    "                    if prefix in prefix_container:\n",
    "                        d[prefix] += 1\n",
    "\n",
    "                # Checking for opcodes\n",
    "                for code in opcodes:\n",
    "                    for word in rem_line:\n",
    "                        if code == word:\n",
    "                            d[code] += 1\n",
    "\n",
    "                # Checking for registers\n",
    "                for register in registers:\n",
    "                    for word in rem_line:\n",
    "                        if register in word:\n",
    "                            d[register] += 1\n",
    "\n",
    "                # Checking for keywords\n",
    "                for keyword in keywords:\n",
    "                    for word in rem_line:\n",
    "                        if keyword in word:\n",
    "                            d[keyword] += 1\n",
    "\n",
    "\n",
    "        # Appending this count_vect/dict in the list of dicts\t\n",
    "        count_vects.append(d)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(count_vects)\n",
    "    df = df.set_index('Id')\n",
    "    destination = os.path.join(base_dir, 'asm_pro1_large.csv')\n",
    "    if os.path.exists(destination):\n",
    "        os.remove(destination)\n",
    "    df.to_csv(destination)\n",
    "    return \n",
    "\n",
    "\n",
    "def process2():\n",
    "\n",
    "    source = os.path.join(base_dir, 'asm2')\n",
    "\n",
    "    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:'\\\n",
    "                ,'.tls:','.reloc:','.BSS:','.CODE']\n",
    "\n",
    "    opcodes = [\n",
    "        'add','al','bt','call','cdq','cld','cli','cmc','cmp', \\\n",
    "        'const','cwd','daa','db','dd','dec','dw','endp','ends',\\\n",
    "        'faddp','fchs','fdiv','fdivp','fdivr','fild','fistp','fld',\\\n",
    "        'fstcw','fstcwimul','fstp','fword','fxch','imul','in','inc','wait','xchg','xor'\\\n",
    "        'ins','int','jb','je','jg','jge','jl','jmp','jnb','jno','jnz','jo','jz',\\\n",
    "        'lea','loope','mov','movzx','mul','near','neg','not','or','out','outs',\\\n",
    "        'pop','popf','proc','push','pushf','rcl','rcr','rdtsc','rep','ret','retn',\\\n",
    "        'rol','ror','sal','sar','sbb','scas','setb','setle','setnle','setnz',\\\n",
    "        'setz','shl','shld','shr','sidt','stc','std','sti','stos','sub','test',\n",
    "    ]\n",
    "\n",
    "\n",
    "    keywords = ['.dll','std::',':dword', 'Virtual', 'Offset', 'loc', 'ptr', 'data', 'FUNCTION', 'void', 'byte', 'trap', \\\n",
    "                'arg', 'dll', 'stdcall', 'asc', 'align', 'rva', 'proc', 'assume', 'Import', 'extrn', \n",
    "    ]\n",
    "\n",
    "\n",
    "    # virtual in small\n",
    "    # offset in small\n",
    "\n",
    "\n",
    "    registers = [\n",
    "        'edx', 'esi', 'es', 'fs', 'ds', 'ss', 'gs', 'cs', 'ah', 'al', 'ax', 'bh', 'bl', 'bx', 'ch', 'cl',\\\n",
    "        'cx', 'dh', 'dl', 'dx', 'eax', 'ebp', 'ebx', 'ecx', 'edi', 'esp', 'eip'\n",
    "    ]\n",
    "\n",
    "    keys = ['Id'] + prefixes + opcodes + keywords + registers\n",
    "\n",
    "\n",
    "    count_vects = []\n",
    "    for file in tqdm(os.listdir(source)):\n",
    "        \n",
    "        # Initialising the dict/count_vector for this file\n",
    "        d = {key: 0 for key in keys}\n",
    "        d['Id'] = file.split('.')[0]\n",
    "\n",
    "        # Building the dict/count_vector for this file\n",
    "        with codecs.open(os.path.join(source, file), mode='r', encoding='cp1252', errors='ignore') as f:\n",
    "            # Iterating over each line in the file\n",
    "            for line in f.read().split('\\n'):\n",
    "                words = line.split()\n",
    "                if not len(words):\n",
    "                    continue\n",
    "                prefix_container = words[0]\n",
    "                rem_line = words[1:]\n",
    "\n",
    "                # Checking what prefix is there in the container\n",
    "                for prefix in prefixes:\n",
    "                    if prefix in prefix_container:\n",
    "                        d[prefix] += 1\n",
    "\n",
    "                # Checking for opcodes\n",
    "                for code in opcodes:\n",
    "                    for word in rem_line:\n",
    "                        if code == word:\n",
    "                            d[code] += 1\n",
    "\n",
    "                # Checking for registers\n",
    "                for register in registers:\n",
    "                    for word in rem_line:\n",
    "                        if register in word:\n",
    "                            d[register] += 1\n",
    "\n",
    "                # Checking for keywords\n",
    "                for keyword in keywords:\n",
    "                    for word in rem_line:\n",
    "                        if keyword in word:\n",
    "                            d[keyword] += 1\n",
    "\n",
    "\n",
    "        # Appending this count_vect/dict in the list of dicts\t\n",
    "        count_vects.append(d)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(count_vects)\n",
    "    df = df.set_index('Id')\n",
    "    destination = os.path.join(base_dir, 'asm_pro2_large.csv')\n",
    "    if os.path.exists(destination):\n",
    "        os.remove(destination)\n",
    "    df.to_csv(destination)\n",
    "    return\n",
    "\n",
    "\n",
    "def process3():\n",
    "\n",
    "    source = os.path.join(base_dir, 'asm3')\n",
    "\n",
    "\n",
    "    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:'\\\n",
    "                ,'.tls:','.reloc:','.BSS:','.CODE']\n",
    "\n",
    "    opcodes = [\n",
    "        'add','al','bt','call','cdq','cld','cli','cmc','cmp', \\\n",
    "        'const','cwd','daa','db','dd','dec','dw','endp','ends',\\\n",
    "        'faddp','fchs','fdiv','fdivp','fdivr','fild','fistp','fld',\\\n",
    "        'fstcw','fstcwimul','fstp','fword','fxch','imul','in','inc','wait','xchg','xor'\\\n",
    "        'ins','int','jb','je','jg','jge','jl','jmp','jnb','jno','jnz','jo','jz',\\\n",
    "        'lea','loope','mov','movzx','mul','near','neg','not','or','out','outs',\\\n",
    "        'pop','popf','proc','push','pushf','rcl','rcr','rdtsc','rep','ret','retn',\\\n",
    "        'rol','ror','sal','sar','sbb','scas','setb','setle','setnle','setnz',\\\n",
    "        'setz','shl','shld','shr','sidt','stc','std','sti','stos','sub','test',\n",
    "    ]\n",
    "\n",
    "\n",
    "    keywords = ['.dll','std::',':dword', 'Virtual', 'Offset', 'loc', 'ptr', 'data', 'FUNCTION', 'void', 'byte', 'trap', \\\n",
    "                'arg', 'dll', 'stdcall', 'asc', 'align', 'rva', 'proc', 'assume', 'Import', 'extrn', \n",
    "    ]\n",
    "\n",
    "\n",
    "    # virtual in small\n",
    "    # offset in small\n",
    "\n",
    "\n",
    "    registers = [\n",
    "        'edx', 'esi', 'es', 'fs', 'ds', 'ss', 'gs', 'cs', 'ah', 'al', 'ax', 'bh', 'bl', 'bx', 'ch', 'cl',\\\n",
    "        'cx', 'dh', 'dl', 'dx', 'eax', 'ebp', 'ebx', 'ecx', 'edi', 'esp', 'eip'\n",
    "    ]\n",
    "\n",
    "    keys = ['Id'] + prefixes + opcodes + keywords + registers\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    count_vects = []\n",
    "    for file in tqdm(os.listdir(source)):\n",
    "        \n",
    "        # Initialising the dict/count_vector for this file\n",
    "        d = {key: 0 for key in keys}\n",
    "        d['Id'] = file.split('.')[0]\n",
    "\n",
    "        # Building the dict/count_vector for this file\n",
    "        with codecs.open(os.path.join(source, file), mode='r', encoding='cp1252', errors='ignore') as f:\n",
    "            # Iterating over each line in the file\n",
    "            for line in f.read().split('\\n'):\n",
    "                words = line.split()\n",
    "                if not len(words):\n",
    "                    continue\n",
    "                prefix_container = words[0]\n",
    "                rem_line = words[1:]\n",
    "\n",
    "                # Checking what prefix is there in the container\n",
    "                for prefix in prefixes:\n",
    "                    if prefix in prefix_container:\n",
    "                        d[prefix] += 1\n",
    "\n",
    "                # Checking for opcodes\n",
    "                for code in opcodes:\n",
    "                    for word in rem_line:\n",
    "                        if code == word:\n",
    "                            d[code] += 1\n",
    "\n",
    "                # Checking for registers\n",
    "                for register in registers:\n",
    "                    for word in rem_line:\n",
    "                        if register in word:\n",
    "                            d[register] += 1\n",
    "\n",
    "                # Checking for keywords\n",
    "                for keyword in keywords:\n",
    "                    for word in rem_line:\n",
    "                        if keyword in word:\n",
    "                            d[keyword] += 1\n",
    "\n",
    "\n",
    "        # Appending this count_vect/dict in the list of dicts\t\n",
    "        count_vects.append(d)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(count_vects)\n",
    "    df = df.set_index('Id')\n",
    "    destination = os.path.join(base_dir, 'asm_pro3_large.csv')\n",
    "    if os.path.exists(destination):\n",
    "        os.remove(destination)\n",
    "    df.to_csv(destination)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process4():\n",
    "\n",
    "    source = os.path.join(base_dir, 'asm4')\n",
    "\n",
    "    \n",
    "    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:'\\\n",
    "                ,'.tls:','.reloc:','.BSS:','.CODE']\n",
    "\n",
    "    opcodes = [\n",
    "        'add','al','bt','call','cdq','cld','cli','cmc','cmp', \\\n",
    "        'const','cwd','daa','db','dd','dec','dw','endp','ends',\\\n",
    "        'faddp','fchs','fdiv','fdivp','fdivr','fild','fistp','fld',\\\n",
    "        'fstcw','fstcwimul','fstp','fword','fxch','imul','in','inc','wait','xchg','xor'\\\n",
    "        'ins','int','jb','je','jg','jge','jl','jmp','jnb','jno','jnz','jo','jz',\\\n",
    "        'lea','loope','mov','movzx','mul','near','neg','not','or','out','outs',\\\n",
    "        'pop','popf','proc','push','pushf','rcl','rcr','rdtsc','rep','ret','retn',\\\n",
    "        'rol','ror','sal','sar','sbb','scas','setb','setle','setnle','setnz',\\\n",
    "        'setz','shl','shld','shr','sidt','stc','std','sti','stos','sub','test',\n",
    "    ]\n",
    "\n",
    "\n",
    "    keywords = ['.dll','std::',':dword', 'Virtual', 'Offset', 'loc', 'ptr', 'data', 'FUNCTION', 'void', 'byte', 'trap', \\\n",
    "                'arg', 'dll', 'stdcall', 'asc', 'align', 'rva', 'proc', 'assume', 'Import', 'extrn', \n",
    "    ]\n",
    "\n",
    "\n",
    "    # virtual in small\n",
    "    # offset in small\n",
    "\n",
    "\n",
    "    registers = [\n",
    "        'edx', 'esi', 'es', 'fs', 'ds', 'ss', 'gs', 'cs', 'ah', 'al', 'ax', 'bh', 'bl', 'bx', 'ch', 'cl',\\\n",
    "        'cx', 'dh', 'dl', 'dx', 'eax', 'ebp', 'ebx', 'ecx', 'edi', 'esp', 'eip'\n",
    "    ]\n",
    "\n",
    "    keys = ['Id'] + prefixes + opcodes + keywords + registers\n",
    "\n",
    "\n",
    "    count_vects = []\n",
    "    for file in tqdm(os.listdir(source)):\n",
    "        \n",
    "        # Initialising the dict/count_vector for this file\n",
    "        d = {key: 0 for key in keys}\n",
    "        d['Id'] = file.split('.')[0]\n",
    "\n",
    "        # Building the dict/count_vector for this file\n",
    "        with codecs.open(os.path.join(source, file), mode='r', encoding='cp1252', errors='ignore') as f:\n",
    "            # Iterating over each line in the file\n",
    "            for line in f.read().split('\\n'):\n",
    "                words = line.split()\n",
    "                if not len(words):\n",
    "                    continue\n",
    "                prefix_container = words[0]\n",
    "                rem_line = words[1:]\n",
    "\n",
    "                # Checking what prefix is there in the container\n",
    "                for prefix in prefixes:\n",
    "                    if prefix in prefix_container:\n",
    "                        d[prefix] += 1\n",
    "\n",
    "                # Checking for opcodes\n",
    "                for code in opcodes:\n",
    "                    for word in rem_line:\n",
    "                        if code == word:\n",
    "                            d[code] += 1\n",
    "\n",
    "                # Checking for registers\n",
    "                for register in registers:\n",
    "                    for word in rem_line:\n",
    "                        if register in word:\n",
    "                            d[register] += 1\n",
    "\n",
    "                # Checking for keywords\n",
    "                for keyword in keywords:\n",
    "                    for word in rem_line:\n",
    "                        if keyword in word:\n",
    "                            d[keyword] += 1\n",
    "\n",
    "\n",
    "        # Appending this count_vect/dict in the list of dicts\t\n",
    "        count_vects.append(d)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(count_vects)\n",
    "    df = df.set_index('Id')\n",
    "    destination = os.path.join(base_dir, 'asm_pro4_large.csv')\n",
    "    if os.path.exists(destination):\n",
    "        os.remove(destination)\n",
    "    df.to_csv(destination)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "################# Uncomment below to run: heavy Code ###############\n",
    "def main():\n",
    "    #a = time.time()\n",
    "    #print(f'parent process begins at {time.ctime()}')\n",
    "    #p1 = multiprocessing.Process(target=process1)\n",
    "    #p2 = multiprocessing.Process(target=process2)\n",
    "    #p3 = multiprocessing.Process(target=process3)\n",
    "    #p4 = multiprocessing.Process(target=process4)\n",
    "\n",
    "    #p1.start()\n",
    "    #p2.start()\n",
    "    #p3.start()\n",
    "    #p4.start()\n",
    "\n",
    "    #p1.join()\n",
    "    #p2.join()\n",
    "    #p3.join()\n",
    "    #p4.join()\n",
    "    #print(f'parent process ends at {time.ctime()}')\n",
    "    #b = time.time()\n",
    "    #print(f'time taken = {b - a}')\n",
    "    print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the count vector features have been extracted, all the 4 csv files can be combined into a single csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the 4 asm features files\n",
    "asm1 = pd.read_csv('./input/asm_pro1_large.csv')\n",
    "asm2 = pd.read_csv('./input/asm_pro2_large.csv')\n",
    "asm3 = pd.read_csv('./input/asm_pro3_large.csv')\n",
    "asm4 = pd.read_csv('./input/asm_pro4_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>.BSS:</th>\n",
       "      <th>.CODE</th>\n",
       "      <th>.Pav:</th>\n",
       "      <th>.bss:</th>\n",
       "      <th>.data:</th>\n",
       "      <th>.dll</th>\n",
       "      <th>.edata:</th>\n",
       "      <th>.idata:</th>\n",
       "      <th>.rdata:</th>\n",
       "      <th>...</th>\n",
       "      <th>stdcall</th>\n",
       "      <th>sti</th>\n",
       "      <th>stos</th>\n",
       "      <th>sub</th>\n",
       "      <th>test</th>\n",
       "      <th>trap</th>\n",
       "      <th>void</th>\n",
       "      <th>wait</th>\n",
       "      <th>xchg</th>\n",
       "      <th>xorins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01IsoiSMh5gxyDYTl4CB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24568</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>26405</td>\n",
       "      <td>...</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02IOCvYEy8mjiuAQHax3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77840</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02MRILoE6rNhmt7FUi45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4329</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>606</td>\n",
       "      <td>2349368</td>\n",
       "      <td>...</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04cvLCVPqBMs6yn5xGlE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>783910</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>598</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04QzZ3DVdPsEp9elLR65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1033</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Id  .BSS:  .CODE  .Pav:  .bss:  .data:  .dll  .edata:  \\\n",
       "0  01IsoiSMh5gxyDYTl4CB      0      0      0      0   24568    24        0   \n",
       "1  02IOCvYEy8mjiuAQHax3      0      0      0      0   77840     6        0   \n",
       "2  02MRILoE6rNhmt7FUi45      0      0      0      0    4329    19        0   \n",
       "3  04cvLCVPqBMs6yn5xGlE      0      0      0      0  783910    12        0   \n",
       "4  04QzZ3DVdPsEp9elLR65      0      0      0      0    1033    12        0   \n",
       "\n",
       "   .idata:  .rdata:  ...  stdcall  sti  stos  sub  test  trap  void  wait  \\\n",
       "0      616    26405  ...      266    0     0  216    48    11   255     0   \n",
       "1       41        0  ...       10    0     0    7     1     0     5     0   \n",
       "2      606  2349368  ...      212    0     0  122    61    11   147     0   \n",
       "3      123      598  ...       33    0     0   68     6     0     1     0   \n",
       "4      124      640  ...       33    0     0   64    10     0     2     0   \n",
       "\n",
       "   xchg  xorins  \n",
       "0     0       0  \n",
       "1     4       0  \n",
       "2     0       0  \n",
       "3     0       0  \n",
       "4     1       0  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining the 4 dataframes into one\n",
    "asm_count_features = pd.concat(objs=[asm1, asm2, asm3, asm4])\n",
    "asm_count_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving into csv\n",
    "asm_count_features_tosave = asm_count_features.set_index('Id')\n",
    "asm_count_features_tosave.to_csv('./input/asm_count_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10868, 153)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the csv file\n",
    "asm_count_features = pd.read_csv('./input/asm_count_features.csv')\n",
    "asm_count_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Extracting and Combining asm image features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most interesting features that can be extracted from the asm files are the image features. The asm files when turned into images show interesting similarities among the files of the same malware class. \n",
    "\n",
    "The inspiration behind extracting the first 1000 pixel data comes from the wining solution of the Kaggle Competetion on Microsoft Malware Classification Challenge of 2015. Our sincere thanks to the source of this approach. \n",
    "https://www.youtube.com/watch?v=VLQTRlLGz5Y\n",
    "\n",
    "\n",
    "An asm file when converted and viewed as an image shows interesting similarities between the data points of same malware class.\n",
    "This observation can be tapped to further featurize our asm files apart from the standard NLP based count vectorizing approach.\n",
    "\n",
    "A sample image of an asm file looks like:\n",
    "\n",
    "<img src=\"test.png\">\n",
    "\n",
    "It is observed that the first 1000 pixels of the above kind image contain the most useful information for our objective.\n",
    "Thus, we will be extracting and using the first 1000 pixel data out of the images as our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Script Source for extracting image features from the asm files\n",
    "\n",
    "import os\n",
    "import time\n",
    "import imageio\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from array import array\n",
    "from constants import *\n",
    "\n",
    "\n",
    "def process1():\n",
    "    source = os.path.join(base_dir, 'asm1')\n",
    "\n",
    "    #source = sample_asm_dir\n",
    "\n",
    "    image_feature_keys = [f'img_f{i+1}' for i in range(1000)] \n",
    "    keys = ['Id'] + image_feature_keys \n",
    "\n",
    "    count_vects = []\n",
    "    for file in tqdm(os.listdir(source)):\n",
    "        \n",
    "        # Initialising the dict/count_vector for this file\n",
    "        d = {key: 0 for key in keys}\n",
    "        d['Id'] = file.split('.')[0]\n",
    "\n",
    "        length = os.path.getsize(os.path.join(source, file))\n",
    "        width = int(length**0.5)\n",
    "        remaining = length % width\n",
    "        file_content_bytes = array('B')\n",
    "    \n",
    "        with open(os.path.join(source, file), mode='rb') as f:\n",
    "            file_content_bytes.fromfile(f, length - remaining)\n",
    "\n",
    "        img_features = np.array(file_content_bytes[:1000])\n",
    "\n",
    "        for i in range(1000):\n",
    "            d[f'img_f{i+1}'] = img_features[i]\n",
    "\n",
    "        # Appending this count_vect/dict in the list of dicts\t\n",
    "        count_vects.append(d)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(count_vects)\n",
    "    df = df.set_index('Id')\n",
    "    destination = os.path.join(base_dir, 'asm_img1_large.csv')\n",
    "    if os.path.exists(destination):\n",
    "        os.remove(destination)\n",
    "    df.to_csv(destination)\n",
    "    return \n",
    "\n",
    "\n",
    "def process2():\n",
    "    source = os.path.join(base_dir, 'asm2')\n",
    "\n",
    "    #source = sample_asm_dir\n",
    "\n",
    "    image_feature_keys = [f'img_f{i+1}' for i in range(1000)] \n",
    "    keys = ['Id'] + image_feature_keys \n",
    "\n",
    "    count_vects = []\n",
    "    for file in tqdm(os.listdir(source)):\n",
    "        \n",
    "        # Initialising the dict/count_vector for this file\n",
    "        d = {key: 0 for key in keys}\n",
    "        d['Id'] = file.split('.')[0]\n",
    "\n",
    "        length = os.path.getsize(os.path.join(source, file))\n",
    "        width = int(length**0.5)\n",
    "        remaining = length % width\n",
    "        file_content_bytes = array('B')\n",
    "    \n",
    "        with open(os.path.join(source, file), mode='rb') as f:\n",
    "            file_content_bytes.fromfile(f, length - remaining)\n",
    "\n",
    "        img_features = np.array(file_content_bytes[:1000])\n",
    "\n",
    "        for i in range(1000):\n",
    "            d[f'img_f{i+1}'] = img_features[i]\n",
    "\n",
    "        # Appending this count_vect/dict in the list of dicts\t\n",
    "        count_vects.append(d)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(count_vects)\n",
    "    df = df.set_index('Id')\n",
    "    destination = os.path.join(base_dir, 'asm_img2_large.csv')\n",
    "    if os.path.exists(destination):\n",
    "        os.remove(destination)\n",
    "    df.to_csv(destination)\n",
    "    return\n",
    "\n",
    "\n",
    "def process3():\n",
    "    source = os.path.join(base_dir, 'asm3')\n",
    "\n",
    "    #source = sample_asm_dir\n",
    "\n",
    "    image_feature_keys = [f'img_f{i+1}' for i in range(1000)] \n",
    "    keys = ['Id'] + image_feature_keys \n",
    "\n",
    "    count_vects = []\n",
    "    for file in tqdm(os.listdir(source)):\n",
    "        \n",
    "        # Initialising the dict/count_vector for this file\n",
    "        d = {key: 0 for key in keys}\n",
    "        d['Id'] = file.split('.')[0]\n",
    "\n",
    "        length = os.path.getsize(os.path.join(source, file))\n",
    "        width = int(length**0.5)\n",
    "        remaining = length % width\n",
    "        file_content_bytes = array('B')\n",
    "    \n",
    "        with open(os.path.join(source, file), mode='rb') as f:\n",
    "            file_content_bytes.fromfile(f, length - remaining)\n",
    "\n",
    "        img_features = np.array(file_content_bytes[:1000])\n",
    "\n",
    "        for i in range(1000):\n",
    "            d[f'img_f{i+1}'] = img_features[i]\n",
    "\n",
    "        # Appending this count_vect/dict in the list of dicts\t\n",
    "        count_vects.append(d)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(count_vects)\n",
    "    df = df.set_index('Id')\n",
    "    destination = os.path.join(base_dir, 'asm_img3_large.csv')\n",
    "    if os.path.exists(destination):\n",
    "        os.remove(destination)\n",
    "    df.to_csv(destination)\n",
    "    return\n",
    "\n",
    "\n",
    "def process4():\n",
    "    source = os.path.join(base_dir, 'asm4')\n",
    "\n",
    "    #source = sample_asm_dir\n",
    "\n",
    "    image_feature_keys = [f'img_f{i+1}' for i in range(1000)] \n",
    "    keys = ['Id'] + image_feature_keys \n",
    "\n",
    "    count_vects = []\n",
    "    for file in tqdm(os.listdir(source)):\n",
    "        \n",
    "        # Initialising the dict/count_vector for this file\n",
    "        d = {key: 0 for key in keys}\n",
    "        d['Id'] = file.split('.')[0]\n",
    "\n",
    "        length = os.path.getsize(os.path.join(source, file))\n",
    "        width = int(length**0.5)\n",
    "        remaining = length % width\n",
    "        file_content_bytes = array('B')\n",
    "    \n",
    "        with open(os.path.join(source, file), mode='rb') as f:\n",
    "            file_content_bytes.fromfile(f, length - remaining)\n",
    "\n",
    "        img_features = np.array(file_content_bytes[:1000])\n",
    "\n",
    "        for i in range(1000):\n",
    "            d[f'img_f{i+1}'] = img_features[i]\n",
    "\n",
    "        # Appending this count_vect/dict in the list of dicts\t\n",
    "        count_vects.append(d)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(count_vects)\n",
    "    df = df.set_index('Id')\n",
    "    destination = os.path.join(base_dir, 'asm_img4_large.csv')\n",
    "    if os.path.exists(destination):\n",
    "        os.remove(destination)\n",
    "    df.to_csv(destination)\n",
    "    return\n",
    "    \n",
    "\n",
    "\n",
    "############## uncomment below to run ###############    \n",
    "def main():\n",
    "    #a = time.time()\n",
    "    #print(f'parent process begins at {time.ctime()}')\n",
    "    #p1 = multiprocessing.Process(target=process1)\n",
    "    #p2 = multiprocessing.Process(target=process2)\n",
    "    #p3 = multiprocessing.Process(target=process3)\n",
    "    #p4 = multiprocessing.Process(target=process4)\n",
    "\n",
    "    #p1.start()\n",
    "    #p2.start()\n",
    "    #p3.start()\n",
    "    #p4.start()\n",
    "\n",
    "    #p1.join()\n",
    "    #p2.join()\n",
    "    #p3.join()\n",
    "    #p4.join()\n",
    "    #print(f'parent process ends at {time.ctime()}')\n",
    "    #b = time.time()\n",
    "    #print(f'time taken = {b - a}')\n",
    "    print()\n",
    "\n",
    "def test():\n",
    "    a = time.time()\n",
    "    print(f'parent process begins at {time.ctime()}')\n",
    "    \n",
    "    process1()\n",
    "\n",
    "    print(f'parent process ends at {time.ctime()}')\n",
    "    b = time.time()\n",
    "    print(f'time taken = {b - a}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    #test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the 4 image feature files have been built, they are combined to form a single csv containing the image features of the total 10868 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Loading the 4 image feature files\n",
    "asm_img1 = pd.read_csv('./input/asm_img1_large.csv')\n",
    "asm_img2 = pd.read_csv('./input/asm_img2_large.csv')\n",
    "asm_img3 = pd.read_csv('./input/asm_img3_large.csv')\n",
    "asm_img4 = pd.read_csv('./input/asm_img4_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>img_f1</th>\n",
       "      <th>img_f10</th>\n",
       "      <th>img_f100</th>\n",
       "      <th>img_f1000</th>\n",
       "      <th>img_f101</th>\n",
       "      <th>img_f102</th>\n",
       "      <th>img_f103</th>\n",
       "      <th>img_f104</th>\n",
       "      <th>img_f105</th>\n",
       "      <th>...</th>\n",
       "      <th>img_f990</th>\n",
       "      <th>img_f991</th>\n",
       "      <th>img_f992</th>\n",
       "      <th>img_f993</th>\n",
       "      <th>img_f994</th>\n",
       "      <th>img_f995</th>\n",
       "      <th>img_f996</th>\n",
       "      <th>img_f997</th>\n",
       "      <th>img_f998</th>\n",
       "      <th>img_f999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01IsoiSMh5gxyDYTl4CB</td>\n",
       "      <td>46</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>116</td>\n",
       "      <td>101</td>\n",
       "      <td>120</td>\n",
       "      <td>116</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02IOCvYEy8mjiuAQHax3</td>\n",
       "      <td>72</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>111</td>\n",
       "      <td>114</td>\n",
       "      <td>109</td>\n",
       "      <td>97</td>\n",
       "      <td>116</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02MRILoE6rNhmt7FUi45</td>\n",
       "      <td>46</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>116</td>\n",
       "      <td>101</td>\n",
       "      <td>120</td>\n",
       "      <td>116</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04cvLCVPqBMs6yn5xGlE</td>\n",
       "      <td>72</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>111</td>\n",
       "      <td>114</td>\n",
       "      <td>109</td>\n",
       "      <td>97</td>\n",
       "      <td>116</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04QzZ3DVdPsEp9elLR65</td>\n",
       "      <td>72</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>71</td>\n",
       "      <td>77</td>\n",
       "      <td>69</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>32</td>\n",
       "      <td>72</td>\n",
       "      <td>69</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Id  img_f1  img_f10  img_f100  img_f1000  img_f101  \\\n",
       "0  01IsoiSMh5gxyDYTl4CB      46       48        45         52        45   \n",
       "1  02IOCvYEy8mjiuAQHax3      72       52        45         32        45   \n",
       "2  02MRILoE6rNhmt7FUi45      46       48        45         52        45   \n",
       "3  04cvLCVPqBMs6yn5xGlE      72       52        45         32        45   \n",
       "4  04QzZ3DVdPsEp9elLR65      72       52        45         68        45   \n",
       "\n",
       "   img_f102  img_f103  img_f104  img_f105  ...  img_f990  img_f991  img_f992  \\\n",
       "0        45        45        45        45  ...        13        10        46   \n",
       "1        45        45        45        45  ...        32        59        32   \n",
       "2        45        45        45        45  ...        13        10        46   \n",
       "3        45        45        45        45  ...        32        59        32   \n",
       "4        45        45        45        45  ...        69        71        77   \n",
       "\n",
       "   img_f993  img_f994  img_f995  img_f996  img_f997  img_f998  img_f999  \n",
       "0       116       101       120       116        58        48        48  \n",
       "1        70       111       114       109        97       116         9  \n",
       "2       116       101       120       116        58        48        48  \n",
       "3        70       111       114       109        97       116         9  \n",
       "4        69        78        84        32        72        69        65  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining the 4 image features dataframes into a single df\n",
    "asm_img_features = pd.concat(objs=[asm_img1, asm_img2, asm_img3, asm_img4])\n",
    "asm_img_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Saving into csv\n",
    "asm_img_features_tosave = asm_img_features.set_index('Id')\n",
    "asm_img_features_tosave.to_csv('./input/asm_img_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>img_f1</th>\n",
       "      <th>img_f10</th>\n",
       "      <th>img_f100</th>\n",
       "      <th>img_f1000</th>\n",
       "      <th>img_f101</th>\n",
       "      <th>img_f102</th>\n",
       "      <th>img_f103</th>\n",
       "      <th>img_f104</th>\n",
       "      <th>img_f105</th>\n",
       "      <th>...</th>\n",
       "      <th>img_f990</th>\n",
       "      <th>img_f991</th>\n",
       "      <th>img_f992</th>\n",
       "      <th>img_f993</th>\n",
       "      <th>img_f994</th>\n",
       "      <th>img_f995</th>\n",
       "      <th>img_f996</th>\n",
       "      <th>img_f997</th>\n",
       "      <th>img_f998</th>\n",
       "      <th>img_f999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01IsoiSMh5gxyDYTl4CB</td>\n",
       "      <td>46</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>116</td>\n",
       "      <td>101</td>\n",
       "      <td>120</td>\n",
       "      <td>116</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02IOCvYEy8mjiuAQHax3</td>\n",
       "      <td>72</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>111</td>\n",
       "      <td>114</td>\n",
       "      <td>109</td>\n",
       "      <td>97</td>\n",
       "      <td>116</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02MRILoE6rNhmt7FUi45</td>\n",
       "      <td>46</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>116</td>\n",
       "      <td>101</td>\n",
       "      <td>120</td>\n",
       "      <td>116</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04cvLCVPqBMs6yn5xGlE</td>\n",
       "      <td>72</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>111</td>\n",
       "      <td>114</td>\n",
       "      <td>109</td>\n",
       "      <td>97</td>\n",
       "      <td>116</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04QzZ3DVdPsEp9elLR65</td>\n",
       "      <td>72</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>71</td>\n",
       "      <td>77</td>\n",
       "      <td>69</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>32</td>\n",
       "      <td>72</td>\n",
       "      <td>69</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Id  img_f1  img_f10  img_f100  img_f1000  img_f101  \\\n",
       "0  01IsoiSMh5gxyDYTl4CB      46       48        45         52        45   \n",
       "1  02IOCvYEy8mjiuAQHax3      72       52        45         32        45   \n",
       "2  02MRILoE6rNhmt7FUi45      46       48        45         52        45   \n",
       "3  04cvLCVPqBMs6yn5xGlE      72       52        45         32        45   \n",
       "4  04QzZ3DVdPsEp9elLR65      72       52        45         68        45   \n",
       "\n",
       "   img_f102  img_f103  img_f104  img_f105  ...  img_f990  img_f991  img_f992  \\\n",
       "0        45        45        45        45  ...        13        10        46   \n",
       "1        45        45        45        45  ...        32        59        32   \n",
       "2        45        45        45        45  ...        13        10        46   \n",
       "3        45        45        45        45  ...        32        59        32   \n",
       "4        45        45        45        45  ...        69        71        77   \n",
       "\n",
       "   img_f993  img_f994  img_f995  img_f996  img_f997  img_f998  img_f999  \n",
       "0       116       101       120       116        58        48        48  \n",
       "1        70       111       114       109        97       116         9  \n",
       "2       116       101       120       116        58        48        48  \n",
       "3        70       111       114       109        97       116         9  \n",
       "4        69        78        84        32        72        69        65  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the csv file\n",
    "asm_img_features = pd.read_csv('./input/asm_img_features.csv')\n",
    "asm_img_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Combining all asm features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>.BSS:</th>\n",
       "      <th>.CODE</th>\n",
       "      <th>.Pav:</th>\n",
       "      <th>.bss:</th>\n",
       "      <th>.data:</th>\n",
       "      <th>.dll</th>\n",
       "      <th>.edata:</th>\n",
       "      <th>.idata:</th>\n",
       "      <th>.rdata:</th>\n",
       "      <th>...</th>\n",
       "      <th>img_f991</th>\n",
       "      <th>img_f992</th>\n",
       "      <th>img_f993</th>\n",
       "      <th>img_f994</th>\n",
       "      <th>img_f995</th>\n",
       "      <th>img_f996</th>\n",
       "      <th>img_f997</th>\n",
       "      <th>img_f998</th>\n",
       "      <th>img_f999</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01IsoiSMh5gxyDYTl4CB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24568</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>26405</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>116</td>\n",
       "      <td>101</td>\n",
       "      <td>120</td>\n",
       "      <td>116</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>13.999378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02IOCvYEy8mjiuAQHax3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77840</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>111</td>\n",
       "      <td>114</td>\n",
       "      <td>109</td>\n",
       "      <td>97</td>\n",
       "      <td>116</td>\n",
       "      <td>9</td>\n",
       "      <td>3.296968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02MRILoE6rNhmt7FUi45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4329</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>606</td>\n",
       "      <td>2349368</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>116</td>\n",
       "      <td>101</td>\n",
       "      <td>120</td>\n",
       "      <td>116</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>83.639698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04cvLCVPqBMs6yn5xGlE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>783910</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>598</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>111</td>\n",
       "      <td>114</td>\n",
       "      <td>109</td>\n",
       "      <td>97</td>\n",
       "      <td>116</td>\n",
       "      <td>9</td>\n",
       "      <td>32.699180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04QzZ3DVdPsEp9elLR65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1033</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>77</td>\n",
       "      <td>69</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>32</td>\n",
       "      <td>72</td>\n",
       "      <td>69</td>\n",
       "      <td>65</td>\n",
       "      <td>0.196652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Id  .BSS:  .CODE  .Pav:  .bss:  .data:  .dll  .edata:  \\\n",
       "0  01IsoiSMh5gxyDYTl4CB      0      0      0      0   24568    24        0   \n",
       "1  02IOCvYEy8mjiuAQHax3      0      0      0      0   77840     6        0   \n",
       "2  02MRILoE6rNhmt7FUi45      0      0      0      0    4329    19        0   \n",
       "3  04cvLCVPqBMs6yn5xGlE      0      0      0      0  783910    12        0   \n",
       "4  04QzZ3DVdPsEp9elLR65      0      0      0      0    1033    12        0   \n",
       "\n",
       "   .idata:  .rdata:  ...  img_f991  img_f992  img_f993  img_f994  img_f995  \\\n",
       "0      616    26405  ...        10        46       116       101       120   \n",
       "1       41        0  ...        59        32        70       111       114   \n",
       "2      606  2349368  ...        10        46       116       101       120   \n",
       "3      123      598  ...        59        32        70       111       114   \n",
       "4      124      640  ...        71        77        69        78        84   \n",
       "\n",
       "   img_f996  img_f997  img_f998  img_f999       Size  \n",
       "0       116        58        48        48  13.999378  \n",
       "1       109        97       116         9   3.296968  \n",
       "2       116        58        48        48  83.639698  \n",
       "3       109        97       116         9  32.699180  \n",
       "4        32        72        69        65   0.196652  \n",
       "\n",
       "[5 rows x 1154 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging our 3 dataframes\n",
    "asm_final = asm_count_features.merge(asm_img_features, on='Id')\n",
    "asm_final = asm_final.merge(asm_size_features, on='Id')\n",
    "asm_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Removing Useless Features out from the asm_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function to build the list of useless features in the dataframe\n",
    "# A feature is useless if for all the datapoints, mean, min, max are all 0.0 along that feature.\n",
    "def get_useless_features(df):\n",
    "    useless_features = []\n",
    "    for each in tqdm(df.columns):\n",
    "        if each != 'Id' and (df[each].mean() == 0.0 and df[each].min() == 0.0 and df[each].max() == 0.0 and df[each].std() == 0.0):\n",
    "            #print(f'## {each} is useless ##')\n",
    "            useless_features.append(each)\n",
    "    \n",
    "    return useless_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1154/1154 [00:00<00:00, 5128.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.BSS:', '.CODE', 'fstcwimul', 'je', 'xorins']\n"
     ]
    }
   ],
   "source": [
    "# getting the list of useless features\n",
    "useless_features_asm = get_useless_features(asm_final)\n",
    "print(useless_features_asm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10868, 1149)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping off the useless features from the asm_features df\n",
    "asm_final = asm_final.drop(labels=useless_features_asm, axis=1)\n",
    "asm_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Saving asm_final into csv after dropping off useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Saving into csv\n",
    "asm_final_tosave = asm_final.set_index('Id')\n",
    "asm_final_tosave.to_csv('./input/asm_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>.Pav:</th>\n",
       "      <th>.bss:</th>\n",
       "      <th>.data:</th>\n",
       "      <th>.dll</th>\n",
       "      <th>.edata:</th>\n",
       "      <th>.idata:</th>\n",
       "      <th>.rdata:</th>\n",
       "      <th>.reloc:</th>\n",
       "      <th>.rsrc:</th>\n",
       "      <th>...</th>\n",
       "      <th>img_f991</th>\n",
       "      <th>img_f992</th>\n",
       "      <th>img_f993</th>\n",
       "      <th>img_f994</th>\n",
       "      <th>img_f995</th>\n",
       "      <th>img_f996</th>\n",
       "      <th>img_f997</th>\n",
       "      <th>img_f998</th>\n",
       "      <th>img_f999</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01IsoiSMh5gxyDYTl4CB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24568</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>26405</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>116</td>\n",
       "      <td>101</td>\n",
       "      <td>120</td>\n",
       "      <td>116</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>13.999378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02IOCvYEy8mjiuAQHax3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77840</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>111</td>\n",
       "      <td>114</td>\n",
       "      <td>109</td>\n",
       "      <td>97</td>\n",
       "      <td>116</td>\n",
       "      <td>9</td>\n",
       "      <td>3.296968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02MRILoE6rNhmt7FUi45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4329</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>606</td>\n",
       "      <td>2349368</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>116</td>\n",
       "      <td>101</td>\n",
       "      <td>120</td>\n",
       "      <td>116</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>83.639698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04cvLCVPqBMs6yn5xGlE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>783910</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>598</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>111</td>\n",
       "      <td>114</td>\n",
       "      <td>109</td>\n",
       "      <td>97</td>\n",
       "      <td>116</td>\n",
       "      <td>9</td>\n",
       "      <td>32.699180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04QzZ3DVdPsEp9elLR65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1033</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>640</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>77</td>\n",
       "      <td>69</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>32</td>\n",
       "      <td>72</td>\n",
       "      <td>69</td>\n",
       "      <td>65</td>\n",
       "      <td>0.196652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Id  .Pav:  .bss:  .data:  .dll  .edata:  .idata:  \\\n",
       "0  01IsoiSMh5gxyDYTl4CB      0      0   24568    24        0      616   \n",
       "1  02IOCvYEy8mjiuAQHax3      0      0   77840     6        0       41   \n",
       "2  02MRILoE6rNhmt7FUi45      0      0    4329    19        0      606   \n",
       "3  04cvLCVPqBMs6yn5xGlE      0      0  783910    12        0      123   \n",
       "4  04QzZ3DVdPsEp9elLR65      0      0    1033    12        0      124   \n",
       "\n",
       "   .rdata:  .reloc:  .rsrc:  ...  img_f991  img_f992  img_f993  img_f994  \\\n",
       "0    26405        0       0  ...        10        46       116       101   \n",
       "1        0        0       3  ...        59        32        70       111   \n",
       "2  2349368        0       0  ...        10        46       116       101   \n",
       "3      598        0       0  ...        59        32        70       111   \n",
       "4      640        0       3  ...        71        77        69        78   \n",
       "\n",
       "   img_f995  img_f996  img_f997  img_f998  img_f999       Size  \n",
       "0       120       116        58        48        48  13.999378  \n",
       "1       114       109        97       116         9   3.296968  \n",
       "2       120       116        58        48        48  83.639698  \n",
       "3       114       109        97       116         9  32.699180  \n",
       "4        84        32        72        69        65   0.196652  \n",
       "\n",
       "[5 rows x 1149 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the csv\n",
    "asm_final = pd.read_csv('./input/asm_final.csv')\n",
    "asm_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
